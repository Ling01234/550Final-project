@article{alon_code2vec,
  author     = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  title      = {Code2Vec: Learning Distributed Representations of Code},
  journal    = {Proc. ACM Program. Lang.},
  issue_date = {January 2019},
  volume     = {3},
  number     = {POPL},
  month      = jan,
  year       = {2019},
  issn       = {2475-1421},
  pages      = {40:1--40:29},
  articleno  = {40},
  numpages   = {29},
  url        = {http://doi.acm.org/10.1145/3290353},
  doi        = {10.1145/3290353},
  acmid      = {3290353},
  publisher  = {ACM},
  address    = {New York, NY, USA},
  keywords   = {Big Code, Distributed Representations, Machine Learning}
}

@inproceedings{code_transformer,
  title     = {Language-Agnostic Representation Learning of Source Code from Structure and Context},
  author    = {Z{\"u}gner, Daniel and Kirschstein, Tobias and Catasta, Michele and Leskovec, Jure and G{\"u}nnemann, Stephan},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021}
}

@article{clone_detection,
  title       = {Code Clone Detection using Code2Vec},
  author      = {Prasad, A.},
  year        = {2020},
  institution = {UC Irvine},
  note        = {ProQuest ID: Prasad\_uci\_0030M\_16699. Merritt ID: ark:/13030/m5wb1h48. Retrieved from https://escholarship.org/uc/item/5qx4b1xh}
}


@inproceedings{clone_ast,
  author    = {I. D. Baxter and A. Yahin and L. Moura and M. Sant'Anna and L. Bier},
  title     = {Clone Detection Using Abstract Syntax Trees},
  booktitle = {Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272)},
  year      = {1998},
  pages     = {368--377},
  doi       = {10.1109/ICSM.1998.738528},
  address   = {Bethesda, MD, USA}
}

@article{5:transformer_code_token,
  title    = {Efficient transformer with code token learner for code clone detection},
  journal  = {Journal of Systems and Software},
  volume   = {197},
  pages    = {111557},
  year     = {2023},
  issn     = {0164-1212},
  doi      = {https://doi.org/10.1016/j.jss.2022.111557},
  url      = {https://www.sciencedirect.com/science/article/pii/S0164121222002333},
  author   = {Aiping Zhang and Liming Fang and Chunpeng Ge and Piji Li and Zhe Liu},
  keywords = {Code clone detection, Code token learner, Efficient transformer},
  abstract = {Deep learning techniques have achieved promising results in code clone detection in the past decade. Unfortunately, current deep learning-based methods rarely explicitly consider the modeling of long codes. Worse, the code length is increasing due to the increasing requirement of complex functions. Thus, modeling the relationship between code tokens to catch their long-range dependencies is crucial to comprehensively capture the information of the code fragment. In this work, we resort to the Transformer to capture long-range dependencies within a code, which however requires huge computational cost for long code fragments. To make it possible to apply Transformer efficiently, we propose a code token learner to largely reduce the number of feature tokens in an automatic way. Besides, considering the tree structure of the abstract syntax tree, we present a tree-based position embedding to encode the position of each token in the input. Apart from the Transformer that captures the dependency within a code, we further leverage a cross-code attention module to capture the similarities between two code fragments. Our method significantly reduces the computational cost of using Transformer by 97% while achieves superior performance with state-of-the-art methods. Our code is available at https://github.com/ArcticHare105/Code-Token-Learner.}
}

@inproceedings{6,
  author    = {Goller, Christoph and KÃ¼chler, Andreas},
  title     = {Learning task-dependent distributed representations by backpropagation through structure},
  booktitle = {ICNN'96},
  year      = {1996}
}

@article{7,
  author  = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  title   = {Efficient estimation of word representations in vector space},
  journal = {arXiv preprint arXiv:1301.3781},
  year    = {2013}
}
